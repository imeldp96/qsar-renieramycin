{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5ebc32-630f-4c15-9402-50ef141d2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('./data250824.csv', index_col=[0], header=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0869ea91-1bbf-47ed-8ac3-11b97de84919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "feature_sets = {\n",
    "    'vi-qc': ['X3', 'X2', 'X1', 'X17'],\n",
    "    'ga-qc': ['X2', 'X4', 'X15', 'X16'],\n",
    "    'vi-ms': ['M43', 'M58', 'M2', 'M42'],\n",
    "    'ga-ms': ['M2', 'M52', 'M67', 'M102']\n",
    "}\n",
    "\n",
    "# Define models with tuned hyperparameters (FOR WHOLE DATA SET)\n",
    "tuned_hyperparameters = {\n",
    "    'vi-qc': {\n",
    "        'RF': {'bootstrap': False, 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 10, 'min_samples_split': 11, 'n_estimators': 99, 'random_state': 42, 'n_jobs': -1},\n",
    "        'SVR': {'C': 124.34299606015858, 'epsilon': 0.1436663234855287, 'kernel': 'rbf'},\n",
    "        'XGB': {'booster':'gbtree', 'device':'cpu', 'objective':'reg:squarederror', 'verbosity': 2, 'tree_method':'auto', 'seed': 42, 'n_jobs': -1,\n",
    "                'colsample_bytree': 0.10108222695147716, 'gamma': 0.03195015716291206, 'learning_rate': 0.1532942849179548, 'max_depth': 3, 'min_child_weight': 2.5672988836952646, 'reg_alpha': 0.28393875667236645, 'reg_lambda': 8.471227306156468, 'subsample': 0.36934305158488395}\n",
    "    },\n",
    "    'ga-qc': {\n",
    "        'RF': {'bootstrap': True, 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 59, 'random_state': 42, 'n_jobs': -1},\n",
    "        'SVR': {'C': 1.0, 'epsilon': 0.1, 'kernel': 'rbf'},\n",
    "        'XGB': {'booster':'gbtree', 'objective':'reg:squarederror', 'verbosity': 2, 'tree_method':'auto', 'seed': 42, 'n_jobs': -1,\n",
    "                'colsample_bytree': 0.1360716562727799, 'gamma': 0.16137390404554144, 'learning_rate': 0.2727409061252117, 'max_depth': 2, 'min_child_weight': 3.9511645755552687, 'reg_alpha': 0.16804646499331374, 'reg_lambda': 8.104479048297152, 'subsample': 0.6070548406804517},\n",
    "        'MLR': {}\n",
    "    },\n",
    "    'vi-ms': {\n",
    "        'RF': {'bootstrap': True, 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 147, 'random_state': 42, 'n_jobs': -1},\n",
    "        'SVR': {'C': 1.6028197376373123, 'degree': 4, 'epsilon': 0.0, 'kernel': 'rbf'},\n",
    "        'XGB': {'booster':'gbtree', 'device':'cpu', 'objective':'reg:squarederror', 'verbosity': 2, 'tree_method':'auto', 'seed': 42, 'validate_parameters': True, 'n_jobs': -1,\n",
    "                'colsample_bytree': 0.9915139189418589, 'gamma': 0.7183487649345512, 'learning_rate': 0.04234226319867907, 'max_depth': 6, 'min_child_weight': 0.7111973058597089, 'reg_alpha': 0.002473915667967762, 'reg_lambda': 0.2952577589021902, 'subsample': 0.33404173513149854}\n",
    "    },\n",
    "    'ga-ms': {\n",
    "        'RF': {'bootstrap': True, 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 126, 'random_state': 42, 'n_jobs': -1},\n",
    "        'SVR': {'C': 4.752986507554117, 'epsilon': 0.21223832828344527, 'kernel': 'rbf'},\n",
    "        'XGB': {'booster':'gbtree', 'device':'cpu', 'objective':'reg:squarederror', 'verbosity': 2, 'tree_method':'auto', 'seed': 42, 'validate_parameters': True, 'n_jobs': -1,\n",
    "                },\n",
    "        'MLR': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Target variable\n",
    "y = data['pIC50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61564089-5ef2-4735-86b1-da7f67ed512c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature Set Model   R2 Mean    R2 Std  RMSE Mean  RMSE Std\n",
      "0        vi-qc    RF  0.679058  0.148348   0.842178  0.188834\n",
      "1        vi-qc   SVR  0.797652  0.173722   0.660444  0.182959\n",
      "2        vi-qc   XGB  0.843489  0.055205   0.596184  0.088515\n",
      "3        ga-qc    RF  0.898543  0.038073   0.478465  0.080982\n",
      "4        ga-qc   SVR  0.849571  0.045533   0.584615  0.085893\n",
      "5        ga-qc   XGB  0.851972  0.063384   0.575967  0.108822\n",
      "6        ga-qc   MLR  0.886959  0.039051   0.507394  0.070181\n",
      "7        vi-ms    RF  0.909801  0.057402   0.439669  0.126675\n",
      "8        vi-ms   SVR  0.860346  0.071991   0.554435  0.129403\n",
      "9        vi-ms   XGB  0.846827  0.080160   0.581403  0.132259\n",
      "10       ga-ms    RF  0.853346  0.073353   0.569187  0.128130\n",
      "11       ga-ms   SVR  0.859606  0.065439   0.556472  0.127280\n",
      "12       ga-ms   XGB  0.853226  0.091514   0.559110  0.167527\n",
      "13       ga-ms   MLR  0.900775  0.031752   0.475884  0.061969\n"
     ]
    }
   ],
   "source": [
    "#FOR TRAINING SET\n",
    "from sklearn.utils import resample\n",
    "\n",
    "n_bootstraps = 1000\n",
    "boot_results = []\n",
    "mlr_coefs_collection = []\n",
    "all_metrics_records = []  # ✅ New list to store per-bootstrap metrics\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for feature_set_name, features in feature_sets.items():\n",
    "    X_train = data[features].iloc[:22].values\n",
    "    y_train = y.iloc[:22].values\n",
    "\n",
    "    for model_name, hyperparams in tuned_hyperparameters[feature_set_name].items():\n",
    "        if model_name == 'RF':\n",
    "            model = RandomForestRegressor(**hyperparams)\n",
    "            scale = False\n",
    "        elif model_name == 'SVR':\n",
    "            model = SVR(**hyperparams)\n",
    "            scale = True\n",
    "        elif model_name == 'XGB':\n",
    "            model = XGBRegressor(**hyperparams)\n",
    "            scale = False\n",
    "        elif model_name == 'MLR':\n",
    "            model = LinearRegression()\n",
    "            scale = False\n",
    "\n",
    "        metrics = {'r2': [], 'rmse': []}\n",
    "        coefs = []\n",
    "        intercepts = []\n",
    "\n",
    "        for i in range(n_bootstraps):\n",
    "            X_boot, y_boot = resample(X_train, y_train)\n",
    "\n",
    "            if scale:\n",
    "                X_boot_scaled = scaler.fit_transform(X_boot)\n",
    "                X_train_scaled = scaler.transform(X_train)\n",
    "                model.fit(X_boot_scaled, y_boot)\n",
    "                y_pred = model.predict(X_train_scaled)\n",
    "            else:\n",
    "                model.fit(X_boot, y_boot)\n",
    "                y_pred = model.predict(X_train)\n",
    "                if model_name == 'MLR':\n",
    "                    coefs.append(model.coef_)\n",
    "                    intercepts.append(model.intercept_)  # ✅ Save intercept\n",
    "            \n",
    "            r2 = r2_score(y_train, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "\n",
    "            metrics['r2'].append(r2)\n",
    "            metrics['rmse'].append(rmse)\n",
    "\n",
    "            # ✅ Append per-bootstrap metric result\n",
    "            all_metrics_records.append({\n",
    "                'Feature Set': feature_set_name,\n",
    "                'Model': model_name,\n",
    "                'Bootstrap': i,\n",
    "                'R2': r2,\n",
    "                'RMSE': rmse\n",
    "            })\n",
    "\n",
    "        result = {\n",
    "            'Feature Set': feature_set_name,\n",
    "            'Model': model_name,\n",
    "            'R2 Mean': np.mean(metrics['r2']),\n",
    "            'R2 Std': np.std(metrics['r2']),\n",
    "            'RMSE Mean': np.mean(metrics['rmse']),\n",
    "            'RMSE Std': np.std(metrics['rmse']),\n",
    "        }\n",
    "\n",
    "        if model_name == 'MLR':\n",
    "            # Save all individual coefficients and intercepts\n",
    "            coef_df = pd.DataFrame(coefs, columns=features)\n",
    "            coef_df['Intercept'] = intercepts  # ✅ Add intercept column\n",
    "            coef_df['Feature Set'] = feature_set_name\n",
    "            mlr_coefs_collection.append(coef_df)\n",
    "\n",
    "        boot_results.append(result)\n",
    "\n",
    "# ✅ Save summary metrics\n",
    "results_df = pd.DataFrame(boot_results)\n",
    "results_df.to_csv('./bootstrapping_eval_metrics_training.csv', index=False)\n",
    "print(results_df)\n",
    "\n",
    "# ✅ Save all bootstrap metrics\n",
    "metrics_df = pd.DataFrame(all_metrics_records)\n",
    "metrics_df.to_csv('./bootstrapped_all_metrics.csv', index=False)\n",
    "\n",
    "# ✅ Save all MLR coefficients\n",
    "if mlr_coefs_collection:\n",
    "    all_mlr_coefs_df = pd.concat(mlr_coefs_collection, ignore_index=True)\n",
    "    all_mlr_coefs_df.to_csv('./bootstrapped_mlr_coefficients.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "996994e4-c904-4c51-acb2-8c0b8c7bdf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature Set Model   R2 Mean     R2 Std  RMSE Mean  RMSE Std\n",
      "0        vi-qc    RF  0.299637   4.481071   0.688038  0.239556\n",
      "1        vi-qc   SVR  0.905175   0.295517   0.307279  0.069566\n",
      "2        vi-qc   XGB -0.118110   7.239649   0.874901  0.320571\n",
      "3        ga-qc    RF  0.939620   0.208517   0.234014  0.079401\n",
      "4        ga-qc   SVR  0.906578   0.415638   0.272189  0.080926\n",
      "5        ga-qc   XGB  0.694506   1.878848   0.403690  0.137822\n",
      "6        ga-qc   MLR -0.152965  22.814389   0.451665  0.089268\n",
      "7        vi-ms    RF  0.779058   1.034794   0.422757  0.147735\n",
      "8        vi-ms   SVR  0.130907   8.937917   0.538784  0.167166\n",
      "9        vi-ms   XGB  0.250703   3.053004   0.788631  0.318567\n",
      "10       ga-ms    RF  0.686549   3.565466   0.313499  0.089890\n",
      "11       ga-ms   SVR  0.935113   0.252058   0.256122  0.099720\n",
      "12       ga-ms   XGB  0.849361   3.067491   0.213711  0.036915\n",
      "13       ga-ms   MLR -0.433482  22.533211   0.262163  0.026137\n"
     ]
    }
   ],
   "source": [
    "#FOR TEST SET\n",
    "\n",
    "n_bootstraps = 1000\n",
    "boot_results = []\n",
    "all_metrics_records = []  # ✅ Store per-bootstrap results\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rng = np.random.default_rng()  # Make sure RNG is defined\n",
    "\n",
    "for feature_set_name, features in feature_sets.items():\n",
    "    X_train = data[features].iloc[:22].values\n",
    "    y_train = y.iloc[:22].values\n",
    "\n",
    "    X_test = data[features].iloc[22:28].values\n",
    "    y_test = y.iloc[22:28].values\n",
    "\n",
    "    for model_name, hyperparams in tuned_hyperparameters[feature_set_name].items():\n",
    "        if model_name == 'RF':\n",
    "            model = RandomForestRegressor(**hyperparams)\n",
    "            scale = False\n",
    "        elif model_name == 'SVR':\n",
    "            model = SVR(**hyperparams)\n",
    "            scale = True\n",
    "        elif model_name == 'XGB':\n",
    "            model = XGBRegressor(**hyperparams)\n",
    "            scale = False\n",
    "        elif model_name == 'MLR':\n",
    "            model = LinearRegression()\n",
    "            scale = False\n",
    "\n",
    "        metrics = {'r2': [], 'rmse': []}\n",
    "\n",
    "        # Train once on full training data\n",
    "        if scale:\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "        for i in range(n_bootstraps):\n",
    "            X_boot, y_boot = resample(X_test, y_test, random_state=rng.integers(0, 1e6))\n",
    "\n",
    "            if scale:\n",
    "                y_pred = model.predict(scaler.transform(X_boot))\n",
    "            else:\n",
    "                y_pred = model.predict(X_boot)\n",
    "\n",
    "            r2 = r2_score(y_boot, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_boot, y_pred))\n",
    "\n",
    "            metrics['r2'].append(r2)\n",
    "            metrics['rmse'].append(rmse)\n",
    "\n",
    "            # ✅ Save individual bootstrap result\n",
    "            all_metrics_records.append({\n",
    "                'Feature Set': feature_set_name,\n",
    "                'Model': model_name,\n",
    "                'Bootstrap': i,\n",
    "                'R2': r2,\n",
    "                'RMSE': rmse\n",
    "            })\n",
    "\n",
    "        boot_results.append({\n",
    "            'Feature Set': feature_set_name,\n",
    "            'Model': model_name,\n",
    "            'R2 Mean': np.mean(metrics['r2']),\n",
    "            'R2 Std': np.std(metrics['r2']),\n",
    "            'RMSE Mean': np.mean(metrics['rmse']),\n",
    "            'RMSE Std': np.std(metrics['rmse'])\n",
    "        })\n",
    "\n",
    "# ✅ Save summary results\n",
    "results_df = pd.DataFrame(boot_results)\n",
    "results_df.to_csv('./bootstrapping_eval_metrics_test.csv', index=False)\n",
    "print(results_df)\n",
    "\n",
    "# ✅ Save all individual bootstrap results\n",
    "metrics_df = pd.DataFrame(all_metrics_records)\n",
    "metrics_df.to_csv('./bootstrapped_all_metrics_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7123330a-c462-4da7-920f-39a48edf1486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction intervals for RF with vi-qc.\n",
      "Saved prediction intervals for SVR with vi-qc.\n",
      "Saved prediction intervals for XGB with vi-qc.\n",
      "Saved prediction intervals for RF with ga-qc.\n",
      "Saved prediction intervals for SVR with ga-qc.\n",
      "Saved prediction intervals for XGB with ga-qc.\n",
      "Saved prediction intervals for MLR with ga-qc.\n",
      "Saved prediction intervals for RF with vi-ms.\n",
      "Saved prediction intervals for SVR with vi-ms.\n",
      "Saved prediction intervals for XGB with vi-ms.\n",
      "Saved prediction intervals for RF with ga-ms.\n",
      "Saved prediction intervals for SVR with ga-ms.\n",
      "Saved prediction intervals for XGB with ga-ms.\n",
      "Saved prediction intervals for MLR with ga-ms.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def bootstrap_prediction_intervals(\n",
    "    X_train, y_train, X_test, model, n_bootstraps=1000, ci=95):\n",
    "    \n",
    "    preds = np.zeros((n_bootstraps, len(X_test)))\n",
    "    residual_stds = []\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # Resample\n",
    "        X_res, y_res = resample(X_train, y_train)\n",
    "        model_clone = clone(model)\n",
    "        model_clone.fit(X_res, y_res)\n",
    "\n",
    "        # Predict on test\n",
    "        preds[i] = model_clone.predict(X_test)\n",
    "\n",
    "        # Residuals on bootstrap train\n",
    "        y_pred_train = model_clone.predict(X_res)\n",
    "        residuals = y_res - y_pred_train\n",
    "        residual_stds.append(np.std(residuals, ddof=1))\n",
    "\n",
    "    # Final predicted mean\n",
    "    mean_preds = np.mean(preds, axis=0)\n",
    "    \n",
    "    # Compute lower and upper prediction interval bounds\n",
    "    alpha = (100 - ci) / 2\n",
    "    lower_percentile = np.percentile(preds, alpha, axis=0)\n",
    "    upper_percentile = np.percentile(preds, 100 - alpha, axis=0)\n",
    "\n",
    "    # Add residual noise (pointwise)\n",
    "    avg_residual_std = np.mean(residual_stds)\n",
    "    lower_bound = lower_percentile - avg_residual_std\n",
    "    upper_bound = upper_percentile + avg_residual_std\n",
    "\n",
    "    return mean_preds, lower_bound, upper_bound\n",
    "\n",
    "n_bootstraps = 1000\n",
    "ci = 95\n",
    "\n",
    "# Split datasets\n",
    "X_train_full = data.iloc[:22]\n",
    "y_train = y.iloc[:22].values\n",
    "X_test_full = data.iloc[22:28]\n",
    "y_test = y.iloc[22:28].values\n",
    "X_val_full = data.iloc[28:42]\n",
    "y_val = y.iloc[28:42].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for feature_set_name, features in feature_sets.items():\n",
    "    X_train = X_train_full[features].values\n",
    "    X_test = X_test_full[features].values\n",
    "    X_val = X_val_full[features].values\n",
    "\n",
    "    for model_name, hyperparams in tuned_hyperparameters[feature_set_name].items():\n",
    "        if model_name == 'RF':\n",
    "            model = RandomForestRegressor(**hyperparams)\n",
    "            scale = False\n",
    "        elif model_name == 'SVR':\n",
    "            model = SVR(**hyperparams)\n",
    "            scale = True\n",
    "        elif model_name == 'XGB':\n",
    "            model = XGBRegressor(**hyperparams)\n",
    "            scale = False\n",
    "        elif model_name == 'MLR':\n",
    "            model = LinearRegression()\n",
    "            scale = False\n",
    "\n",
    "        if scale:\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "        else:\n",
    "            X_train_scaled = X_train\n",
    "            X_test_scaled = X_test\n",
    "            X_val_scaled = X_val\n",
    "\n",
    "        # Compute prediction intervals for test set\n",
    "        test_preds, test_lower, test_upper = bootstrap_prediction_intervals(\n",
    "            X_train_scaled, y_train, X_test_scaled, model, n_bootstraps=n_bootstraps, ci=ci\n",
    "        )\n",
    "\n",
    "        # Compute prediction intervals for validation set\n",
    "        val_preds, val_lower, val_upper = bootstrap_prediction_intervals(\n",
    "            X_train_scaled, y_train, X_val_scaled, model, n_bootstraps=n_bootstraps, ci=ci\n",
    "        )\n",
    "\n",
    "        # Save test results\n",
    "        test_df = pd.DataFrame({\n",
    "            'y_true': y_test,\n",
    "            'y_pred': test_preds,\n",
    "            'lower_bound': test_lower,\n",
    "            'upper_bound': test_upper\n",
    "        })\n",
    "        test_df['Feature Set'] = feature_set_name\n",
    "        test_df['Model'] = model_name\n",
    "        #test_df.to_csv(f'./pred_interval_test_{feature_set_name}_{model_name}.csv', index=False)\n",
    "\n",
    "        # Save validation results\n",
    "        val_df = pd.DataFrame({\n",
    "            'y_true': y_val,\n",
    "            'y_pred': val_preds,\n",
    "            'lower_bound': val_lower,\n",
    "            'upper_bound': val_upper\n",
    "        })\n",
    "        val_df['Feature Set'] = feature_set_name\n",
    "        val_df['Model'] = model_name\n",
    "        val_df.to_csv(f'./pred_interval_val_{feature_set_name}_{model_name}.csv', index=False)\n",
    "\n",
    "        print(f\"Saved prediction intervals for {model_name} with {feature_set_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef7ac5d-ba9f-440c-b68a-3845b8e818ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Set Model  Bootstrap      R2CV    RMSECV\n",
      "0       vi-qc    RF          1  0.675376  0.868025\n",
      "1       vi-qc    RF          2  0.645618  0.906938\n",
      "2       vi-qc    RF          3  0.847641  0.594670\n",
      "3       vi-qc    RF          4  0.726399  0.796894\n",
      "4       vi-qc    RF          5  0.470066  1.109055\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "boot_records = []\n",
    "\n",
    "for feature_set_name, features in feature_sets.items():\n",
    "    X = data[features]\n",
    "    X_orig = X.iloc[:22]\n",
    "    y_orig = y.iloc[:22]\n",
    "\n",
    "    # Compute Total Sum of Squares (TSS) once on original y\n",
    "    y_mean = np.mean(y_orig)\n",
    "    tss_orig = np.sum((y_orig - y_mean) ** 2)\n",
    "\n",
    "    for model_name, hyperparams in tuned_hyperparameters.get(feature_set_name, {}).items():\n",
    "        for b in range(n_bootstraps):\n",
    "            # Bootstrap sampling\n",
    "            X_boot, y_boot = resample(X_orig, y_orig, random_state=rng)\n",
    "\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            e_square = []\n",
    "\n",
    "            loo = LeaveOneOut()\n",
    "\n",
    "            for train_index, test_index in loo.split(X_boot):\n",
    "                X_train_cv, X_test_cv = X_boot.iloc[train_index], X_boot.iloc[test_index]\n",
    "                y_train_cv, y_test_cv = y_boot.iloc[train_index], y_boot.iloc[test_index]\n",
    "\n",
    "                if model_name == 'RF':\n",
    "                    model = RandomForestRegressor(**hyperparams)\n",
    "                elif model_name == 'SVR':\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_cv = scaler.fit_transform(X_train_cv)\n",
    "                    X_test_cv = scaler.transform(X_test_cv)\n",
    "                    model = SVR(**hyperparams)\n",
    "                elif model_name == 'XGB':\n",
    "                    model = XGBRegressor(**hyperparams)\n",
    "                elif model_name == 'MLR':\n",
    "                    model = LinearRegression()\n",
    "\n",
    "                model.fit(X_train_cv, y_train_cv)\n",
    "                prediction = model.predict(X_test_cv)[0]\n",
    "\n",
    "                error_sq = (y_test_cv.values[0] - prediction) ** 2\n",
    "                y_true.append(y_test_cv.values[0])\n",
    "                y_pred.append(prediction)\n",
    "                e_square.append(error_sq)\n",
    "\n",
    "            press = np.sum(e_square)\n",
    "            r2_cv = 1 - (press / tss_orig)\n",
    "            rmse_cv = np.sqrt(np.mean(e_square))\n",
    "\n",
    "            boot_records.append({\n",
    "                'Feature Set': feature_set_name,\n",
    "                'Model': model_name,\n",
    "                'Bootstrap': b + 1,\n",
    "                'R2CV': r2_cv,\n",
    "                'RMSECV': rmse_cv\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "boot_df = pd.DataFrame(boot_records)\n",
    "boot_df.to_csv('./bootstrapped_loocv_results.csv', index=False)\n",
    "print(boot_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27d0e9e7-f2b4-4704-8b7c-334029be16b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction intervals for RF with vi-qc.\n",
      "Saved prediction intervals for SVR with vi-qc.\n",
      "Saved prediction intervals for XGB with vi-qc.\n",
      "Saved prediction intervals for RF with ga-qc.\n",
      "Saved prediction intervals for SVR with ga-qc.\n",
      "Saved prediction intervals for XGB with ga-qc.\n",
      "Saved prediction intervals for MLR with ga-qc.\n",
      "Saved prediction intervals for RF with vi-ms.\n",
      "Saved prediction intervals for SVR with vi-ms.\n",
      "Saved prediction intervals for XGB with vi-ms.\n",
      "Saved prediction intervals for RF with ga-ms.\n",
      "Saved prediction intervals for SVR with ga-ms.\n",
      "Saved prediction intervals for XGB with ga-ms.\n",
      "Saved prediction intervals for MLR with ga-ms.\n"
     ]
    }
   ],
   "source": [
    "#prediction interval for train\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def bootstrap_prediction_intervals(\n",
    "    X_train, y_train, X_test, model, n_bootstraps=1000, ci=95):\n",
    "    \n",
    "    preds = np.zeros((n_bootstraps, len(X_test)))\n",
    "    residual_stds = []\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # Resample\n",
    "        X_res, y_res = resample(X_train, y_train)\n",
    "        model_clone = clone(model)\n",
    "        model_clone.fit(X_res, y_res)\n",
    "\n",
    "        # Predict on test\n",
    "        preds[i] = model_clone.predict(X_test)\n",
    "\n",
    "        # Residuals on bootstrap train\n",
    "        y_pred_train = model_clone.predict(X_res)\n",
    "        residuals = y_res - y_pred_train\n",
    "        residual_stds.append(np.std(residuals, ddof=1))\n",
    "\n",
    "    # Final predicted mean\n",
    "    mean_preds = np.mean(preds, axis=0)\n",
    "    \n",
    "    # Compute lower and upper prediction interval bounds\n",
    "    alpha = (100 - ci) / 2\n",
    "    lower_percentile = np.percentile(preds, alpha, axis=0)\n",
    "    upper_percentile = np.percentile(preds, 100 - alpha, axis=0)\n",
    "\n",
    "    # Add residual noise (pointwise)\n",
    "    avg_residual_std = np.mean(residual_stds)\n",
    "    lower_bound = lower_percentile - avg_residual_std\n",
    "    upper_bound = upper_percentile + avg_residual_std\n",
    "\n",
    "    return mean_preds, lower_bound, upper_bound\n",
    "\n",
    "n_bootstraps = 1000\n",
    "ci = 95\n",
    "\n",
    "# Split datasets\n",
    "X_train_full = data.iloc[:22]\n",
    "y_train = y.iloc[:22].values\n",
    "X_test_full = data.iloc[22:28]\n",
    "y_test = y.iloc[22:28].values\n",
    "X_val_full = data.iloc[28:42]\n",
    "y_val = y.iloc[28:42].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for feature_set_name, features in feature_sets.items():\n",
    "    X_train = X_train_full[features].values\n",
    "    X_test = X_test_full[features].values\n",
    "    X_val = X_val_full[features].values\n",
    "\n",
    "    for model_name, hyperparams in tuned_hyperparameters[feature_set_name].items():\n",
    "        if model_name == 'RF':\n",
    "            model = RandomForestRegressor(**hyperparams)\n",
    "            scale = False\n",
    "        elif model_name == 'SVR':\n",
    "            model = SVR(**hyperparams)\n",
    "            scale = True\n",
    "        elif model_name == 'XGB':\n",
    "            model = XGBRegressor(**hyperparams)\n",
    "            scale = False\n",
    "        elif model_name == 'MLR':\n",
    "            model = LinearRegression()\n",
    "            scale = False\n",
    "\n",
    "        if scale:\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "        else:\n",
    "            X_train_scaled = X_train\n",
    "            X_test_scaled = X_test\n",
    "            X_val_scaled = X_val\n",
    "\n",
    "        # Compute prediction intervals for train set\n",
    "        train_preds, train_lower, train_upper = bootstrap_prediction_intervals(\n",
    "            X_train_scaled, y_train, X_train_scaled, model, n_bootstraps=n_bootstraps, ci=ci\n",
    "        )\n",
    "\n",
    "\n",
    "        # Save test results\n",
    "        train_df = pd.DataFrame({\n",
    "            'y_true': y_train,\n",
    "            'y_pred': train_preds,\n",
    "            'lower_bound': train_lower,\n",
    "            'upper_bound': train_upper\n",
    "        })\n",
    "        train_df['Feature Set'] = feature_set_name\n",
    "        train_df['Model'] = model_name\n",
    "        train_df.to_csv(f'./pred_interval_train_{feature_set_name}_{model_name}.csv', index=False)\n",
    "\n",
    "\n",
    "        print(f\"Saved prediction intervals for {model_name} with {feature_set_name}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
